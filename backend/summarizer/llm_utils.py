# This file is now a simple bridge to the real LLM implementation.
from backend.llm.llm_node import get_llm_summary

def summarize_code_with_llm(code_text: str, question: str) -> str:
    """
    Acts as a pass-through to the actual LLM implementation.

    This function takes the code and a specific question (prompt) and
    forwards them to the centralized LLM handler, which is now configured
    to use the Gemini API. This replaces the previous mock implementation.

    Args:
        code_text: The string of code to be summarized.
        question: The specific question or prompt to guide the summarization.
                  (e.g., "Summarize this function in two sentences.")

    Returns:
        The summary string generated by the language model.
    """
    # The core logic is now delegated to the get_llm_summary function,
    # which handles the actual API call to Gemini.
    return get_llm_summary(question=question, code_context=code_text)
